{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras import Model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.xception import Xception\n",
    "\n",
    "import h5py\n",
    "\n",
    "xception_weights = '../../DeepSlice/metadata/weights/xception_weights_tf_dim_ordering_tf_kernels.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allen_weight_path = '../../DeepSlice/metadata/weights/Allen_Mixed_Best.h5'\n",
    "new_allen_weight_path = '../../DeepSlice/metadata/weights/Allen_Mixed_Best_2.h5'\n",
    "temp_allen_weight_path = '../../DeepSlice/metadata/weights/Allen_Mixed_Best_temp.h5'\n",
    "\n",
    "with h5py.File(temp_allen_weight_path,'w') as f_dest:\n",
    "    with h5py.File(allen_weight_path,'r') as f_src:\n",
    "\n",
    "        layer_names = []\n",
    "\n",
    "        for x in f_src['xception'].items():\n",
    "            print(x[0])\n",
    "            name = x[0]\n",
    "            #Batch normalization layers appear to have flipped beta and gamma\n",
    "\n",
    "            f_src.copy(f_src['xception'][name],f_dest,name)\n",
    "\n",
    "            weight_names = []\n",
    "            for y in f_src['xception'][name].items():\n",
    "                weight_name = y[0]\n",
    "                weight_names.append(weight_name)\n",
    "\n",
    "            #Identify batchnorm layer becuase beta and gamma are flipped for some reason\n",
    "            if 'gamma:0' in weight_names:\n",
    "\n",
    "                gamma_index = weight_names.index('gamma:0')\n",
    "                beta_index = weight_names.index('beta:0')\n",
    "\n",
    "                gamma = f_src['xception'][name]['gamma:0'][...]\n",
    "                beta = f_src['xception'][name]['beta:0'][...]\n",
    "\n",
    "                f_dest[name]['gamma:0'][...] = beta\n",
    "                f_dest[name]['beta:0'][...] = gamma\n",
    "\n",
    "                #weight_names[gamma_index] = 'beta:0'\n",
    "                #weight_names[beta_index] = 'gamma:0'\n",
    "            \n",
    "\n",
    "            f_dest[name].attrs['weight_names']=weight_names\n",
    "\n",
    "            layer_names.append(name)\n",
    "\n",
    "        for x in f_src['xception'].attrs.keys():\n",
    "            #create attribute in destination file\n",
    "            f_dest.attrs['layer_names'] = layer_names\n",
    "\n",
    "\n",
    "base_model = Xception(include_top=True, weights=xception_weights)\n",
    "\n",
    "base_model = Model(inputs=base_model.input, outputs=base_model.layers[-1].output,name=\"xception\")\n",
    "\n",
    "base_model.load_weights(temp_allen_weight_path,skip_mismatch=True,by_name=True)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(base_model)\n",
    "model.add(Dense(256, activation=\"relu\"))\n",
    "model.add(Dense(256, activation=\"relu\"))\n",
    "model.add(Dense(9, activation=\"linear\"))\n",
    "\n",
    "model.load_weights(allen_weight_path,by_name=True,skip_mismatch=True)\n",
    "\n",
    "model.save_weights(new_allen_weight_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_weight_path = '../../DeepSlice/metadata/weights/Synthetic_data_final.hdf5'\n",
    "new_synthetic_weight_path = '../../DeepSlice/metadata/weights/Synthetic_data_final_2.hdf5'\n",
    "temp_synthetic_weight_path = '../../DeepSlice/metadata/weights/Synthetic_data_final_temp.hdf5'\n",
    "\n",
    "with h5py.File(temp_synthetic_weight_path,'w') as f_dest:\n",
    "    with h5py.File(synthetic_weight_path,'r') as f_src:\n",
    "\n",
    "        layer_names = []\n",
    "\n",
    "        for x in f_src['xception'].items():\n",
    "            name = x[0]\n",
    "            f_src.copy(f_src['xception'][name],f_dest,name)\n",
    "\n",
    "            weight_names = []\n",
    "            for x in f_src['xception'][name].items():\n",
    "                weight_name = x[0]\n",
    "                weight_names.append(weight_name)\n",
    "\n",
    "            #Identify batchnorm layer becuase beta and gamma are flipped for some reason\n",
    "            if 'gamma:0' in weight_names:\n",
    "                gamma_index = weight_names.index('gamma:0')\n",
    "                beta_index = weight_names.index('beta:0')\n",
    "\n",
    "                gamma = f_src['xception'][name]['gamma:0'][...]\n",
    "                beta = f_src['xception'][name]['beta:0'][...]\n",
    "\n",
    "                f_dest[name]['gamma:0'][...] = beta\n",
    "                f_dest[name]['beta:0'][...] = gamma\n",
    "\n",
    "                #weight_names[gamma_index] = 'beta:0'\n",
    "                #weight_names[beta_index] = 'gamma:0'\n",
    "            \n",
    "            f_dest[name].attrs['weight_names']=weight_names\n",
    "\n",
    "            layer_names.append(name)\n",
    "\n",
    "        for x in f_src['xception'].attrs.keys():\n",
    "            #create attribute in destination file\n",
    "            f_dest.attrs['layer_names'] = layer_names\n",
    "\n",
    "base_model = Xception(include_top=True, weights=xception_weights)\n",
    "\n",
    "base_model = Model(inputs=base_model.input, outputs=base_model.output,name=\"xception\")\n",
    "\n",
    "model = Sequential()\n",
    "model.add(base_model)\n",
    "model.add(Dense(256, activation=\"relu\"))\n",
    "model.add(Dense(256, activation=\"relu\"))\n",
    "model.add(Dense(9, activation=\"linear\"))\n",
    "\n",
    "base_model.load_weights(temp_synthetic_weight_path,by_name=True)\n",
    "model.load_weights(synthetic_weight_path,by_name=True,skip_mismatch=True)\n",
    "\n",
    "model.save_weights(new_synthetic_weight_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This will not produce a weight file from xception because it doesn't appropriately save teh \n",
    "#weight list\n",
    "\n",
    "\n",
    "synthetic_weight_path = '../../DeepSlice/metadata/weights/Synthetic_data_final.hdf5'\n",
    "new_synthetic_weight_path = '../../DeepSlice/metadata/weights/Synthetic_data_final_2.hdf5'\n",
    "\n",
    "with h5py.File(new_synthetic_weight_path,'w') as f_dest:\n",
    "    with h5py.File(synthetic_weight_path,'r') as f_src:\n",
    "        for x in f_src['xception'].items():\n",
    "            name = x[0]\n",
    "            f_src.copy(f_src['xception'][name],f_dest,name)\n",
    "\n",
    "base_model = Xception(include_top=True, weights=xception_weights)\n",
    "\n",
    "base_model = Model(inputs=base_model.input, outputs=base_model.layers[-1].output,name=\"xception\")\n",
    "\n",
    "model = Sequential()\n",
    "model.add(base_model)\n",
    "model.add(Dense(256, activation=\"relu\"))\n",
    "model.add(Dense(256, activation=\"relu\"))\n",
    "model.add(Dense(9, activation=\"linear\"))\n",
    "\n",
    "base_model.load_weights(new_synthetic_weight_path,by_name=True)\n",
    "\n",
    "model.load_weights(synthetic_weight_path,by_name=True,skip_mismatch=True)\n",
    "\n",
    "model.save_weights(new_synthetic_weight_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
